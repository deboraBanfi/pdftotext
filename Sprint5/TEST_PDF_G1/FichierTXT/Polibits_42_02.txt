Titre: 
Summary Evaluation
 with and without References
 Abstract: 
the evaluation of text summarization systems without
human models which is used to produce system rankings.
The research is carried out using a new content-based
evaluation framework called F RESA to compute a variety of
divergences among probability distributions. We apply our
comparison framework to various well-established content-based
evaluation measures in text summarization such as C OVERAGE,
R ESPONSIVENESS, P YRAMIDS and ROUGE studying their
associations in various text summarization tasks including
generic multi-document summarization in English and French,
focus-based multi-document summarization in English and
generic single-document summarization in French and Spanish.
Index Terms—Text summarization evaluation, content-based
evaluation measures, divergences.

I. I NTRODUCTION
Intro: 

T

EXT summarization evaluation has always been a
complex and controversial issue in computational
linguistics. In the last decade, significant advances have been
made in this field as well as various evaluation measures have
been designed. Two evaluation campaigns have been led by
the U.S. agence DARPA. The first one, SUMMAC, ran from
1996 to 1998 under the auspices of the Tipster program [1],
and the second one, entitled DUC (Document Understanding
Conference) [2], was the main evaluation forum from 2000
until 2007. Nowadays, the Text Analysis Conference (TAC)
[3] provides a forum for assessment of different information
access technologies including text summarization.
Evaluation in text summarization can be extrinsic or
intrinsic [4]. In an extrinsic evaluation, the summaries are
assessed in the context of an specific task carried out by a
human or a machine. In an intrinsic evaluation, the summaries
are evaluated in reference to some ideal model. SUMMAC
was mainly extrinsic while DUC and TAC followed an
intrinsic evaluation paradigm. In an intrinsic evaluation, an
Manuscript received June 8, 2010. Manuscript accepted for publication July
25, 2010.
Juan-Manuel Torres-Moreno is with LIA/Université d’Avignon,
France
and
École
Polytechnique
de
Montréal,
Canada
(juan-manuel.torres@univ-avignon.fr).
Eric
SanJuan
is
with
LIA/Université
d’Avignon,
France
(eric.sanjuan@univ-avignon.fr).
Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain
(horacio.saggion@upf.edu).
Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain;
LIA/Université d’Avignon, France and Instituto de Ingenierı́a/UNAM, Mexico
(iria.dacunha@upf.edu).
Patricia
Velázquez-Morales
is
with
VM
Labs,
France
(patricia velazquez@yahoo.com).

automatically generated summary (peer) has to be compared
with one or more reference summaries (models). DUC used
an interface called SEE to allow human judges to compare
a peer with a model. Thus, judges give a C OVERAGE score
to each peer produced by a system and the final system
C OVERAGE score is the average of the C OVERAGE’s scores
asigned. These system’s C OVERAGE scores can then be used
to rank summarization systems. In the case of query-focused
summarization (e.g. when the summary should answer a
question or series of questions) a R ESPONSIVENESS score
is also assigned to each summary, which indicates how
responsive the summary is to the question(s).
Because manual comparison of peer summaries with model
summaries is an arduous and costly process, a body of
research has been produced in the last decade on automatic
content-based evaluation procedures. Early studies used text
similarity measures such as cosine similarity (with or without
weighting schema) to compare peer and model summaries
[5]. Various vocabulary overlap measures such as n-grams
overlap or longest common subsequence between peer and
model have also been proposed [6], [7]. The B LEU machine
translation evaluation measure [8] has also been tested in
summarization [9]. The DUC conferences adopted the ROUGE
package for content-based evaluation [10]. ROUGE implements
a series of recall measures based on n-gram co-occurrence
between a peer summary and a set of model summaries. These
measures are used to produce systems’ rank. It has been shown
that system rankings, produced by some ROUGE measures
(e.g., ROUGE-2, which uses 2-grams), have a correlation with
rankings produced using C OVERAGE.
In recent years the P YRAMIDS evaluation method [11] has
been introduced. It is based on the distribution of “content”
of a set of model summaries. Summary Content Units (SCUs)
are first identified in the model summaries, then each SCU
receives a weight which is the number of models containing
or expressing the same unit. Peer SCUs are identified in the
peer, matched against model SCUs, and weighted accordingly.
The P YRAMIDS score given to a peer is the ratio of the sum
of the weights of its units and the sum of the weights of the
best possible ideal summary with the same number of SCUs as
the peer. The P YRAMIDS scores can be also used for ranking
summarization systems. [11] showed that P YRAMIDS scores
produced reliable system rankings when multiple (4 or more)
models were used and that P YRAMIDS rankings correlate with
rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE
with skip 2-grams). However, this method requires the creation

Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

of models and the identification, matching, and weighting of
SCUs in both: models and peers.
[12] evaluated the effectiveness of the Jensen-Shannon
(J S) [13] theoretic measure in predicting systems ranks
in two summarization tasks: query-focused and update
summarization. They have shown that ranks produced
by P YRAMIDS and those produced by J S measure
correlate. However, they did not investigate the effect
of the measure in summarization tasks such as generic
multi-document summarization (DUC 2004 Task 2),
biographical summarization (DUC 2004 Task 5), opinion
summarization (TAC 2008 OS), and summarization in
languages other than English.
In this paper we present a series of experiments aimed at
a better understanding of the value of the J S divergence
for ranking summarization systems. We have carried out
experimentation with the proposed measure and we have
verified that in certain tasks (such as those studied by
[12]) there is a strong correlation among P YRAMIDS,
R ESPONSIVENESS and the J S divergence, but as we will
show in this paper, there are datasets in which the correlation
is not so strong. We also present experiments in Spanish
and French showing positive correlation between the J S
and ROUGE which is the de facto evaluation measure used
in evaluation of non-English summarization. To the best of
our knowledge this is the more extensive set of experiments
interpreting the value of evaluation without human models.
The rest of the paper is organized in the following way:
First in Section II we introduce related work in the area of
content-based evaluation identifying the departing point for
our inquiry; then in Section III we explain the methodology
adopted in our work and the tools and resources used for
experimentation. In Section IV we present the experiments
carried out together with the results. Section V discusses the
results and Section VI concludes the paper and identifies future
work.
II. R ELATED W ORK
Corps: 
Summary Evaluation
with and without References
References: 

TABLE VI
S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE PISTES C ORPUS (F RENCH )
Mesure
JS
J S2
J S4
J SM

ROUGE -1
0.70
0.93
0.83
0.88

p-value
p < 0.050
p < 0.002
p < 0.020
p < 0.010

ROUGE -2
0.73
0.86
0.76
0.83

p-value
p < 0.05
p < 0.01
p < 0.05
p < 0.02

ROUGE -SU4
0.73
0.86
0.76
0.83

p-value
p < 0.500
p < 0.005
p < 0.050
p < 0.010

TABLE VII
S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE RPM2 C ORPUS (F RENCH )
Measure
JS
J S2
J S4
J SM

ROUGE -1
0.830
0.800
0.750
0.850

p-value
p < 0.002
p < 0.005
p < 0.010
p < 0.002

ROUGE -2
0.660
0.590
0.520
0.640

R EFERENCES
[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim, “Summac: a text summarization evaluation,” Natural
Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.
[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,
no. 6, pp. 1506–1520, 2007.
[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,
USA: NIST, November 17-19 2008.
[4] K. Spärck Jones and J. Galliers, Evaluating Natural Language
Processing Systems, An Analysis and Review, ser. Lecture Notes in
Computer Science. Springer, 1996, vol. 1083.
[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of
rankings produced by summarization evaluation measures,” in NAACL
Workshop on Automatic Summarization, 2000, pp. 69–78.
[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation
of Summaries in a Cross-lingual Environment using Content-based
Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.
[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,
D. Liu, and E. Drábek, “Evaluation challenges in large-scale document
summarization,” in ACL’03, 2003, pp. 375–382.
[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
311–318.
[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
Initiatives in Natural Language Processing. Budapest, Hungary: EACL,
14 April 2003.
[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,
M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.
[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.
145–152.
[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
in Summarization without Human Models,” in Empirical Methods in
Natural Language Processing, Singapore, August 2009, pp. 306–314.
[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
Transactions on Information Theory, vol. 37, no. 145-151, 1991.
[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
USA: Association for Computational Linguistics, 2003, pp. 71–78.
[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
approach to automatic evaluation of summaries,” in HLT-NAACL,
Morristown, USA, 2006, pp. 463–470.
[16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. of
Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.
[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill, 1998.

p-value
p < 0.05
p < 0.05
p < 0.10
p < 0.05

ROUGE -SU4
0.741
0.680
0.620
0.740

p-value
p < 0.01
p < 0.02
p < 0.05
p < 0.01

[18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
“A French Human Reference Corpus for multi-documents
summarization and sentence compression,” in LREC’10, vol. 2,
Malta, 2010, p. In press.
[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy
of Associative Memories: performants applications of Enertex algorithm
in text summarization and topic segmentation,” in MICAI’07, 2007, pp.
861–871.
[20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G. Meunier,
“Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2,
St Malo, France, 2002, pp. 723–734.
[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales,
“Automatic summarization using terminological and semantic
resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.
[22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton
appliqué au résumé automatique de texte,” in JADT’10. Rome, 2010,
p. In press.
[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern
systems of automatic text summarization,” Automatic Documentation
and Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.
[24] C. D. Manning and H. Schütze, Foundations of Statistical Natural
Language Processing.
Cambridge, Massachusetts: The MIT Press,
1999.
[25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM,
vol. 43, no. 6, pp. 1449–1481, 2007.
[26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specialized
discourse: The case of medical articles in spanish,” Terminology, vol. 13,
no. 2, pp. 249–286, 2007.
[27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL
Student Research Workshop.
Toulouse, France: Association for
Computational Linguistics, 9-11 July 2001 2001, pp. 49–54.
[28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:
Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,
Singapore, August 2009, pp. 23–30.
[29] K. Knight and D. Marcu, “Statistics-based summarization-step one:
Sentence compression,” in Proceedings of the National Conference on
Artificial Intelligence. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999, 2000, pp. 703–710.

Discussion: 
Conclusion: 
